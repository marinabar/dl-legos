{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMP1DK+QYBZAizApoTqyyfF"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dot-Product Self-Attention Mechanism"
      ],
      "metadata": {
        "id": "FWER2BFqCG2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementing the world known algorithm from [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) !!"
      ],
      "metadata": {
        "id": "Q4NZAEPpdEy_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> used in transformer architecture\n",
        "\n",
        "\n",
        "\n",
        "Self attention is needed for quantifying the importance of a word in a sentence.\n",
        "\n",
        "Therefore, each word in a sentence is represented by a vector which depends on the other words. It creates a **context-based embedding**.\n",
        "\n",
        "This is a Python implementation for the mathematical operations behind self-attention."
      ],
      "metadata": {
        "id": "D3CGrplxCLar"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "When we represent a sentence, we represent it by using the biaised representations from each word's point of view. That word is the query."
      ],
      "metadata": {
        "id": "4OaWbS7lg7mY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use three weights matrices **Wq, Wk and Wv**, representing queries, keys and values. We want to find the strength of the relationship of one word to another, therefore the similarity of two vectors, one which is the query, and the other the key.\n",
        "\n",
        "We obtain each of those vectors by multiplying the encoded word by the according matrix"
      ],
      "metadata": {
        "id": "Swmr68GYgFdD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "> `the context vector is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key`\n",
        "\n",
        "directly sourced from *Attention is all you need*\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2gepxZgXcjau"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# example input sentence to tokenize\n",
        "sentence = \"The sofa does not care about board games\"\n",
        "\n",
        "# tokens\n",
        "wdict={w:t for t,w in enumerate(sentence.split())}\n",
        "\n",
        "# sentence tensor\n",
        "sentence_t=torch.tensor([wdict[w] for w in sentence.split()])\n",
        "sentence_t"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tzPjYh5Eve3",
        "outputId": "92fb317a-b8e7-4260-9aa0-41b0235304ad"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 5, 6, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#suppose we have 10 dimensional word encoding (usually in small LLMS it is at least ~~50 or 100)\n",
        "em_layer= torch.nn.Embedding(len(sentence), 10)\n",
        "sentence_repr = em_layer(sentence_t)\n",
        "sentence_repr.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcRBrtMMRRuh",
        "outputId": "f8d57cf7-6157-4115-bd0a-48dd3c9d6417"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([8, 10])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each word is represented by a query vector, which is like its individual representation, and its key vector, which would be a more open representation.\n",
        "\n",
        "This can be identified to query/key pairs in a search.\n",
        "\n",
        "To obtain the attention vector for a given word, we compute the dot product of its query matrix with the key matrices of all the words. This gives us the weight matrix.\n",
        "\n"
      ],
      "metadata": {
        "id": "CBwIOH40SiXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the key and query weight matrices need to be of the same size. But the value matrix does not.\n",
        "\n",
        "The number of rows is equal to the dimension of the words' embedding"
      ],
      "metadata": {
        "id": "7mmTqvIFT1W0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we generate them randomly, in reality the weights are learned\n",
        "\n",
        "Wq=torch.rand(10,18)\n",
        "Wk=torch.rand(10, 18)\n",
        "Wv=torch.rand(10, 22)"
      ],
      "metadata": {
        "id": "meFbe5NiUlLV"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's compute the attention for the first word\n",
        "q1=torch.matmul(sentence_repr[0], Wq)\n",
        "k1=torch.matmul(sentence_repr[0], Wk)\n",
        "v1=torch.matmul(sentence_repr[0], Wv)"
      ],
      "metadata": {
        "id": "FaWqLlwQShXy"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We compute the similarity of a query and each key"
      ],
      "metadata": {
        "id": "ZHSHu5LGb1Sd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To compute vector similarity, is also possible to alter a bit the algorithm here, for example by taking the function similarity (like cosine similarity) :of the key and value matrices instead of the dot product."
      ],
      "metadata": {
        "id": "R3u_FxOrELAV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# let's obtain the key and value vectors for the other words\n",
        "keys=torch.matmul(sentence_repr, Wk)\n",
        "values=torch.matmul(sentence_repr, Wv)\n",
        "\n",
        "print(keys.shape)\n",
        "\n",
        "# now we compute the dot product of q1 and each of the key in the weighted key matrix\n",
        "q1w=torch.matmul(keys, q1)\n",
        "print(q1w)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJfgKsMwWePe",
        "outputId": "95a1aff0-1a66-4c6b-9c8e-aafcc79d0b1d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 18])\n",
            "tensor([183.8672,  89.1740, -20.8962,  37.1406, 126.8375, 101.9559, -33.7133,\n",
            "         51.4582], grad_fn=<MvBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now normalize this result with the softmax function, it is also common practice to divide beforehand the result by the square root of the key matrix dimension - here 18"
      ],
      "metadata": {
        "id": "4xdngO47YwSz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1w_norm=torch.nn.functional.softmax(q1w/(Wk.shape[1])**0.5, dim=0)"
      ],
      "metadata": {
        "id": "-WS6bavMX4lH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q1w_norm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qv46iO8ra8LF",
        "outputId": "fd978de7-8e92-4f01-d142-b565795095df"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1.0000e+00, 2.0268e-10, 1.0954e-21, 9.5597e-16, 1.4528e-06, 4.1230e-09,\n",
              "        5.3400e-23, 2.7929e-14], grad_fn=<SoftmaxBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's calculate a weighted sum of these weights using the value associated to each word in the value matrice"
      ],
      "metadata": {
        "id": "kfThdqd0aKDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "q1_attention=torch.matmul(q1w_norm, values)\n",
        "q1_attention"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-szTdFVaU0f",
        "outputId": "7c7aaf31-7216-4dd4-ea5f-f7c612e61473"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4.1085, 4.1209, 4.8486, 3.9491, 3.3074, 3.4146, 2.5324, 3.4020, 4.2248,\n",
              "        3.2138, 1.6662, 3.9172, 2.5095, 3.4351, 2.9736, 3.9698, 3.7834, 3.2706,\n",
              "        2.0679, 3.7417, 2.5776, 4.4017], grad_fn=<SqueezeBackward4>)"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have our attention vector for the sequence at the 1st word! We can do that same process for a full matrix with rows of inputs and compute the attention at once."
      ],
      "metadata": {
        "id": "VTugqyr3edPj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MH2bKlBbAxu_"
      },
      "outputs": [],
      "source": []
    }
  ]
}